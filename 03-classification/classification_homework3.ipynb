{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "505d0781-3f5f-433e-bf57-9d9048d3c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-14 20:55:41--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80876 (79K) [text/plain]\n",
      "Saving to: ‘course_lead_scoring.csv’\n",
      "\n",
      "course_lead_scoring 100%[===================>]  78.98K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-10-14 20:55:41 (40.5 MB/s) - ‘course_lead_scoring.csv’ saved [80876/80876]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c28ab-0f27-4464-b6e6-921731185ee5",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Check if the missing values are presented in the features.\n",
    "- If there are missing values:\n",
    "- For categorical features, replace them with 'NA'\n",
    "- For numerical features, replace with with 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3a35ade-bbd3-4a57-9b7f-52753b3caf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../03-classification/course_lead_scoring.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aa8c455-3880-4a4a-80bd-014686f22326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income',\n",
      "       'employment_status', 'location', 'interaction_count', 'lead_score',\n",
      "       'converted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2581fb3e-c4c7-4c95-8e38-5ba550bdf354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column before filling:\n",
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values per column before filling:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcc6e2d7-1754-429c-bd3f-b4dce371f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: Index(['lead_source', 'industry', 'employment_status', 'location'], dtype='object')\n",
      "Numerical of Cloumns: Index(['number_of_courses_viewed', 'annual_income', 'interaction_count',\n",
      "       'lead_score', 'converted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "print('Categorical columns:', cat_cols)\n",
    "print('Numerical of Cloumns:', num_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2c6203b-a0c8-4b22-9e08-1685c222b0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column after filling:\n",
      "lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df[cat_cols] = df[cat_cols].fillna('NA')\n",
    "df[num_cols] = df[num_cols].fillna(0.0)\n",
    "\n",
    "print(\"\\nMissing values per column after filling:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1646086-bff5-4cc0-ae90-1231074f7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb2f79-d348-4e3d-bf11-b5f6a3ffbee7",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "What is the most frequent observation (mode) for the column industry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5276a731-784c-43a6-b6e4-0f67279c746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode of 'industry': retail\n"
     ]
    }
   ],
   "source": [
    "mode_industry = df['industry'].mode()[0]\n",
    "print(\"Mode of 'industry':\", mode_industry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bf188-2674-4fcd-be3c-d52b007ecaa2",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31a37b0b-a1d8-457c-ae03-8d13859b5719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "                           number_of_courses_viewed  annual_income  \\\n",
      "number_of_courses_viewed                  1.000000       0.009770   \n",
      "annual_income                             0.009770       1.000000   \n",
      "interaction_count                        -0.023565       0.027036   \n",
      "lead_score                               -0.004879       0.015610   \n",
      "\n",
      "                          interaction_count  lead_score  \n",
      "number_of_courses_viewed          -0.023565   -0.004879  \n",
      "annual_income                      0.027036    0.015610  \n",
      "interaction_count                  1.000000    0.009888  \n",
      "lead_score                         0.009888    1.000000  \n",
      "Strongest correlation pair: annual_income and interaction_count 0.02703647240481443\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "print(\"Correlation matrix:\\n\", corr_matrix)\n",
    "\n",
    "pairs = {\n",
    "    'interaction_count and lead_score': corr_matrix.loc['interaction_count', 'lead_score'],\n",
    "    'number_of_courses_viewed and lead_score': corr_matrix.loc['number_of_courses_viewed', 'lead_score'],\n",
    "    'number_of_courses_viewed and interaction_count': corr_matrix.loc['number_of_courses_viewed', 'interaction_count'],\n",
    "    'annual_income and interaction_count': corr_matrix.loc['annual_income', 'interaction_count']\n",
    "}\n",
    "\n",
    "strongest_pair = max(pairs, key=lambda k: abs(pairs[k]))\n",
    "print(\"Strongest correlation pair:\", strongest_pair, pairs[strongest_pair])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e264f42-6114-4a96-a33f-6cdcfbcd5a20",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "- Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "- Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "- Make sure that the target value converted is not in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d72a9e00-9264-4ab1-b340-ce2d9acd91c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 877, Validation size: 292, Test size: 293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['converted'])\n",
    "y = df['converted']\n",
    "\n",
    "# 60% train, 20% val, 20% test split\n",
    "X_train_full, X_temp, y_train_full, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(X_train_full)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d15e7-727d-4ee6-8ccc-10b37fc11c58",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "- Calculate the mutual information score between converted and other categorical variables in the dataset. Use the training set only.\n",
    "- Round the scores to 2 decimals using round(score, 2).\n",
    "\n",
    "Which of these variables has the biggest mutual information score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af2662b3-d2a3-43b7-bb97-b8bf1cb74f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information scores:\n",
      "{'industry': np.float64(0.0), 'location': np.float64(0.01), 'lead_source': np.float64(0.0), 'employment_status': np.float64(0.01)}\n",
      "Highest mutual information feature: location\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "cat_features = ['industry', 'location', 'lead_source', 'employment_status']\n",
    "X_train_enc = X_train_full.copy()\n",
    "for col in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    X_train_enc[col] = le.fit_transform(X_train_enc[col].astype(str))\n",
    "\n",
    "mi_scores = mutual_info_classif(X_train_enc[cat_features], y_train_full)\n",
    "mi_dict = dict(zip(cat_features, [round(score, 2) for score in mi_scores]))\n",
    "print(\"Mutual information scores:\")\n",
    "print(mi_dict)\n",
    "\n",
    "max_mi_feature = max(mi_dict, key=mi_dict.get)\n",
    "print(\"Highest mutual information feature:\", max_mi_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811473df-ab17-4873-b98d-86da16ef8c5e",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "- Now let's train a logistic regression.\n",
    "- Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "- Fit the model on the training dataset.\n",
    "- To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "- model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "- Calculate the accuracy on the validation dataset and round it to 2 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f3fba42-0d15-4ebf-bb01-6bba6e24001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e80c12b0-bc7c-4ee7-9249-23b451c93daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "train_cat = encoder.fit_transform(X_train_full[cat_features].fillna('NA'))\n",
    "val_cat = encoder.transform(X_val[cat_features].fillna('NA'))\n",
    "\n",
    "num_features = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "X_train_num = X_train_full[num_features].fillna(0.0).values\n",
    "X_val_num = X_val[num_features].fillna(0.0).values\n",
    "\n",
    "X_train_final = np.concatenate([train_cat, X_train_num], axis=1)\n",
    "X_val_final = np.concatenate([val_cat, X_val_num], axis=1)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_final, y_train_full)\n",
    "y_val_pred = model.predict(X_val_final)\n",
    "val_accuracy = round(accuracy_score(y_val, y_val_pred), 2)\n",
    "print(\"Validation accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a0e08-4939-408a-a1d3-81953cae42d2",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "- Let's find the least useful feature using the feature elimination technique.\n",
    "- Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "- Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "- For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "\n",
    "  Which of following feature has the smallest difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24fb3e54-6697-49f5-a911-fc79588d5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy differences when excluding each feature:\n",
      "{'industry': -0.003150684931506831, 'location': -0.003150684931506831, 'lead_source': 0.010547945205479414, 'employment_status': -0.00657534246575342, 'number_of_courses_viewed': 0.06191780821917803, 'annual_income': -0.11616438356164382, 'interaction_count': 0.06534246575342462, 'lead_score': -0.003150684931506831}\n",
      "Feature with smallest difference: industry\n"
     ]
    }
   ],
   "source": [
    "features_all = cat_features + num_features\n",
    "accuracy_diffs = {}\n",
    "\n",
    "for feature in features_all:\n",
    "    drop_cat = [f for f in cat_features if f != feature]\n",
    "    drop_num = [f for f in num_features if f != feature]\n",
    "\n",
    "    train_cat_drop = encoder.fit_transform(X_train_full[drop_cat].fillna('NA')) if drop_cat else np.empty((len(X_train_full), 0))\n",
    "    val_cat_drop = encoder.transform(X_val[drop_cat].fillna('NA')) if drop_cat else np.empty((len(X_val), 0))\n",
    "\n",
    "    train_num_drop = X_train_full[drop_num].fillna(0.0).values if drop_num else np.empty((len(X_train_full), 0))\n",
    "    val_num_drop = X_val[drop_num].fillna(0.0).values if drop_num else np.empty((len(X_val), 0))\n",
    "\n",
    "    X_train_drop = np.concatenate([train_cat_drop, train_num_drop], axis=1)\n",
    "    X_val_drop = np.concatenate([val_cat_drop, val_num_drop], axis=1)\n",
    "\n",
    "    model_drop = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_drop.fit(X_train_drop, y_train_full)\n",
    "    y_pred_drop = model_drop.predict(X_val_drop)\n",
    "    accuracy_drop = accuracy_score(y_val, y_pred_drop)\n",
    "\n",
    "    accuracy_diffs[feature] = val_accuracy - accuracy_drop\n",
    "\n",
    "print(\"Accuracy differences when excluding each feature:\")\n",
    "print(accuracy_diffs)\n",
    "\n",
    "min_diff_feature = min(accuracy_diffs, key=lambda k: abs(accuracy_diffs[k]))\n",
    "print(\"Feature with smallest difference:\", min_diff_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d13b3-abd6-4b3d-b12f-dbf3f649703b",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "- Now let's train a regularized logistic regression.\n",
    "- Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "- Train models using all the features as in Q4.\n",
    "- Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "\n",
    "Which of these C leads to the best accuracy on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dcfa4145-2df1-4c87-8864-7a60c93eae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracies for different C values:\n",
      "{0.01: 0.743, 0.1: 0.743, 1: 0.743, 10: 0.743, 100: 0.743}\n",
      "Best C value: 0.01 with accuracy: 0.743\n"
     ]
    }
   ],
   "source": [
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "accuracy_per_C = {}\n",
    "\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_final, y_train_full)\n",
    "    y_val_pred = model.predict(X_val_final)\n",
    "    accuracy_per_C[C] = round(accuracy_score(y_val, y_val_pred), 3)\n",
    "\n",
    "print(\"Validation accuracies for different C values:\")\n",
    "print(accuracy_per_C)\n",
    "\n",
    "best_C = max(accuracy_per_C, key=accuracy_per_C.get)\n",
    "print(\"Best C value:\", best_C, \"with accuracy:\", accuracy_per_C[best_C])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d10307-31ea-4a7b-8b91-e0b7b64f0310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
